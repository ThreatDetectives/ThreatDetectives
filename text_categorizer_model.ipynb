{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model - Threat Detector\n",
    "## Python 401d15 - 01/22/2021\n",
    "### By : Hexx King, Lee Thomas, Taylor Johnson and Ryan Pilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRIGGER WARNING! Offensive language and hate speech is visible below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the training data and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Toxicity                                              tweet\n",
       "0         0   @user when a father is dysfunctional and is s...\n",
       "1         0  @user @user thanks for #lyft credit i can't us...\n",
       "2         0                                bihday your majesty\n",
       "3         0  #model   i love u take with u all the time in ...\n",
       "4         0             factsguide: society now    #motivation"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Toxicity</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>bihday your majesty</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# In the `read_csv` function, we have passed a parameter for *encoding*, because our data set contains non-english words that's not supported by the default pandas `read_csv` function. \n",
    "\n",
    "dataset = pd.read_csv('./FinalBalancedDataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Pulling out only the columns we want in the dataset\n",
    "\n",
    "dt_transformed = dataset[['Toxicity', 'tweet']]\n",
    "dt_transformed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Toxicity                                              tweet  \\\n",
       "0         0   @user when a father is dysfunctional and is s...   \n",
       "1         0  @user @user thanks for #lyft credit i can't us...   \n",
       "2         0                                bihday your majesty   \n",
       "3         0  #model   i love u take with u all the time in ...   \n",
       "4         0             factsguide: society now    #motivation   \n",
       "\n",
       "                                tweet_wo_RT_username  \n",
       "0    when a father is dysfunctional and is so sel...  \n",
       "1    thanks for  credit i can't use cause they do...  \n",
       "2                                bihday your majesty  \n",
       "3     i love u take with u all the time in urÃ°Â...  \n",
       "4                        factsguide: society now      "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Toxicity</th>\n      <th>tweet</th>\n      <th>tweet_wo_RT_username</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n      <td>when a father is dysfunctional and is so sel...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n      <td>thanks for  credit i can't use cause they do...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesty</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n      <td>i love u take with u all the time in urÃ°Â...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n      <td>factsguide: society now</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#remove user names by pulling all the characters inbetween \"@\" and \":\"\n",
    "#removes hashtags and their text\n",
    "#removes text starting with http\n",
    "#removes the \"RT\"\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_RT_user(text):\n",
    "    tweet = re.sub(\"@[^\\s]+\", \"\", text)\n",
    "    hashtag = re.sub(\"#[\\w|\\d]+\", \"\", tweet)\n",
    "    remove_http = re.sub(\"(https?[a-zA-Z0-9]+)|(http?[a-zA-Z0-9]+)\", \"\", hashtag)\n",
    "    no_rt = re.sub(\"RT\", \"\", remove_http)\n",
    "    return no_rt\n",
    "\n",
    "dt_transformed['tweet_wo_RT_username'] = dt_transformed['tweet'].apply(lambda x: remove_RT_user(x))\n",
    "dt_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Toxicity                                              tweet  \\\n",
       "0         0   @user when a father is dysfunctional and is s...   \n",
       "1         0  @user @user thanks for #lyft credit i can't us...   \n",
       "2         0                                bihday your majesty   \n",
       "3         0  #model   i love u take with u all the time in ...   \n",
       "4         0             factsguide: society now    #motivation   \n",
       "\n",
       "                                tweet_wo_RT_username  \\\n",
       "0    when a father is dysfunctional and is so sel...   \n",
       "1    thanks for  credit i can't use cause they do...   \n",
       "2                                bihday your majesty   \n",
       "3     i love u take with u all the time in urÃ°Â...   \n",
       "4                        factsguide: society now       \n",
       "\n",
       "                          tweet_wo_RT_username_punct  \n",
       "0    when a father is dysfunctional and is so sel...  \n",
       "1    thanks for  credit i cant use cause they don...  \n",
       "2                                bihday your majesty  \n",
       "3     i love u take with u all the time in urÃ°Â...  \n",
       "4                         factsguide society now      "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Toxicity</th>\n      <th>tweet</th>\n      <th>tweet_wo_RT_username</th>\n      <th>tweet_wo_RT_username_punct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n      <td>when a father is dysfunctional and is so sel...</td>\n      <td>when a father is dysfunctional and is so sel...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n      <td>thanks for  credit i can't use cause they do...</td>\n      <td>thanks for  credit i cant use cause they don...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesty</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n      <td>i love u take with u all the time in urÃ°Â...</td>\n      <td>i love u take with u all the time in urÃ°Â...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n      <td>factsguide: society now</td>\n      <td>factsguide society now</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# removing punctuation\n",
    "\n",
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct=[words for words in text if words not in string.punctuation]\n",
    "    words_wo_punct=''.join(no_punct)\n",
    "    return words_wo_punct\n",
    "\n",
    "dt_transformed['tweet_wo_RT_username_punct'] = dt_transformed['tweet_wo_RT_username'].apply(lambda x: remove_punctuation(x))\n",
    "dt_transformed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Toxicity                                              tweet  \\\n",
       "0         0   @user when a father is dysfunctional and is s...   \n",
       "1         0  @user @user thanks for #lyft credit i can't us...   \n",
       "2         0                                bihday your majesty   \n",
       "3         0  #model   i love u take with u all the time in ...   \n",
       "4         0             factsguide: society now    #motivation   \n",
       "\n",
       "                                tweet_wo_RT_username  \\\n",
       "0    when a father is dysfunctional and is so sel...   \n",
       "1    thanks for  credit i can't use cause they do...   \n",
       "2                                bihday your majesty   \n",
       "3     i love u take with u all the time in urÃ°Â...   \n",
       "4                        factsguide: society now       \n",
       "\n",
       "                          tweet_wo_RT_username_punct  \\\n",
       "0    when a father is dysfunctional and is so sel...   \n",
       "1    thanks for  credit i cant use cause they don...   \n",
       "2                                bihday your majesty   \n",
       "3     i love u take with u all the time in urÃ°Â...   \n",
       "4                         factsguide society now       \n",
       "\n",
       "                    tweet_wo_RT_username_punct_split  \n",
       "0  [, when, a, father, is, dysfunctional, and, is...  \n",
       "1  [, thanks, for, credit, i, cant, use, cause, t...  \n",
       "2                          [, bihday, your, majesty]  \n",
       "3  [, i, love, u, take, with, u, all, the, time, ...  \n",
       "4                     [, factsguide, society, now, ]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Toxicity</th>\n      <th>tweet</th>\n      <th>tweet_wo_RT_username</th>\n      <th>tweet_wo_RT_username_punct</th>\n      <th>tweet_wo_RT_username_punct_split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n      <td>when a father is dysfunctional and is so sel...</td>\n      <td>when a father is dysfunctional and is so sel...</td>\n      <td>[, when, a, father, is, dysfunctional, and, is...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n      <td>thanks for  credit i can't use cause they do...</td>\n      <td>thanks for  credit i cant use cause they don...</td>\n      <td>[, thanks, for, credit, i, cant, use, cause, t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesty</td>\n      <td>[, bihday, your, majesty]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n      <td>i love u take with u all the time in urÃ°Â...</td>\n      <td>i love u take with u all the time in urÃ°Â...</td>\n      <td>[, i, love, u, take, with, u, all, the, time, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n      <td>factsguide: society now</td>\n      <td>factsguide society now</td>\n      <td>[, factsguide, society, now, ]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Tokenization = splitting strings into words\n",
    "\n",
    "def tokenize(text):\n",
    "    split = re.split(\"\\W+\", text)\n",
    "    return split\n",
    "\n",
    "dt_transformed['tweet_wo_RT_username_punct_split'] = dt_transformed['tweet_wo_RT_username_punct'].apply(lambda x: tokenize(x))\n",
    "dt_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\"hasn't\", 'be', 'wasn', 'same', 'so', 'doesn', \"shan't\", \"you'll\", 'did', 'am', 'through', \"haven't\", 'having', 'until', 'few', 'at', 'couldn', 'aren', 'out', 'm', 'y', 'then', 'if', 'an', \"it's\", 's', 'all', \"shouldn't\", \"isn't\", 'and', 'hadn', 'doing', 'but', 'her', 'of', \"weren't\", 'will', 'don', 'won', 're', 'further', 'such', \"don't\", \"didn't\", 'in', 'you', 'do', 'does', 'above', 'hasn', 'its', \"you're\", 'where', 'nor', 'ain', 'ourselves', 'their', 'each', 'more', 'themselves', 'other', 'o', 'what', 'the', 'i', 'up', 'for', \"mustn't\", 'now', 'should', \"should've\", 'most', 'about', 'or', 'he', 'with', 'below', \"that'll\", 'from', 'mustn', 'weren', 'just', 'into', \"needn't\", \"you'd\", 'as', 'why', 'both', 'some', \"you've\", 'yourself', \"wasn't\", 'yourselves', 'here', 'than', 'his', 'haven', \"doesn't\", 'before', 'yours', 'over', 'were', 'after', 'only', 'we', 'these', 'was', 'isn', 'down', 'hers', 'too', 'ma', 'this', 've', 'there', 'my', 'can', 'she', 'shan', 'it', 'when', 'because', 'herself', 'has', 'off', \"wouldn't\", 'is', 'under', 'no', 'they', 'between', 'your', 'them', 'on', 'to', \"she's\", 'ours', 'which', 'during', 'very', 'him', 'our', \"mightn't\", 'not', \"aren't\", 'itself', 'own', 'have', \"couldn't\", 'himself', 'are', 'any', 'by', 'once', 'mightn', 'theirs', 'myself', 'a', 'how', 'been', 't', \"won't\", 'whom', 'that', 'had', 'against', 'me', 'again', 'those', 'who', 'being', 'while', 'needn', 'd', 'll', 'didn', 'shouldn', \"hadn't\", 'wouldn'}\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hexking/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading in and looking at the stopwords to be removed from the tweets\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Toxicity                                              tweet  \\\n",
       "0         0   @user when a father is dysfunctional and is s...   \n",
       "1         0  @user @user thanks for #lyft credit i can't us...   \n",
       "2         0                                bihday your majesty   \n",
       "3         0  #model   i love u take with u all the time in ...   \n",
       "4         0             factsguide: society now    #motivation   \n",
       "\n",
       "                                tweet_wo_RT_username  \\\n",
       "0    when a father is dysfunctional and is so sel...   \n",
       "1    thanks for  credit i can't use cause they do...   \n",
       "2                                bihday your majesty   \n",
       "3     i love u take with u all the time in urÃ°Â...   \n",
       "4                        factsguide: society now       \n",
       "\n",
       "                          tweet_wo_RT_username_punct  \\\n",
       "0    when a father is dysfunctional and is so sel...   \n",
       "1    thanks for  credit i cant use cause they don...   \n",
       "2                                bihday your majesty   \n",
       "3     i love u take with u all the time in urÃ°Â...   \n",
       "4                         factsguide society now       \n",
       "\n",
       "                    tweet_wo_RT_username_punct_split  \\\n",
       "0  [, when, a, father, is, dysfunctional, and, is...   \n",
       "1  [, thanks, for, credit, i, cant, use, cause, t...   \n",
       "2                          [, bihday, your, majesty]   \n",
       "3  [, i, love, u, take, with, u, all, the, time, ...   \n",
       "4                     [, factsguide, society, now, ]   \n",
       "\n",
       "          tweet_wo_RT_username_punct_split_stopwords  \n",
       "0  [, father, dysfunctional, selfish, drags, kids...  \n",
       "1  [, thanks, credit, cant, use, cause, dont, off...  \n",
       "2                                [, bihday, majesty]  \n",
       "3  [, love, u, take, u, time, urÃ, Â, Â, Â, Ã, Â,...  \n",
       "4                          [, factsguide, society, ]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Toxicity</th>\n      <th>tweet</th>\n      <th>tweet_wo_RT_username</th>\n      <th>tweet_wo_RT_username_punct</th>\n      <th>tweet_wo_RT_username_punct_split</th>\n      <th>tweet_wo_RT_username_punct_split_stopwords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n      <td>when a father is dysfunctional and is so sel...</td>\n      <td>when a father is dysfunctional and is so sel...</td>\n      <td>[, when, a, father, is, dysfunctional, and, is...</td>\n      <td>[, father, dysfunctional, selfish, drags, kids...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n      <td>thanks for  credit i can't use cause they do...</td>\n      <td>thanks for  credit i cant use cause they don...</td>\n      <td>[, thanks, for, credit, i, cant, use, cause, t...</td>\n      <td>[, thanks, credit, cant, use, cause, dont, off...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesty</td>\n      <td>bihday your majesty</td>\n      <td>[, bihday, your, majesty]</td>\n      <td>[, bihday, majesty]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n      <td>i love u take with u all the time in urÃ°Â...</td>\n      <td>i love u take with u all the time in urÃ°Â...</td>\n      <td>[, i, love, u, take, with, u, all, the, time, ...</td>\n      <td>[, love, u, take, u, time, urÃ, Â, Â, Â, Ã, Â,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n      <td>factsguide: society now</td>\n      <td>factsguide society now</td>\n      <td>[, factsguide, society, now, ]</td>\n      <td>[, factsguide, society, ]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Removing the stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopWords]\n",
    "    return text\n",
    "\n",
    "dt_transformed['tweet_wo_RT_username_punct_split_stopwords'] = dt_transformed['tweet_wo_RT_username_punct_split'].apply(lambda x: remove_stopwords(x))\n",
    "dt_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We have  39771  words in our Bag of Words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing the CountVectorizer to \"vectorize\" sentences by creating a collection of unique words and assigning an index to each one \n",
    "\n",
    "tweets = dt_transformed['tweet_wo_RT_username_punct_split']\n",
    "\n",
    "# `explode()` produces the same as `tweet_list = [item for sublist in tweets for item in sublist]`\n",
    "tweet_list = tweets.explode()\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=None) \n",
    "# `max_features=n` builds a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "vectorizer.fit_transform(tweet_list)\n",
    "# fit_transform is equivalent to fit followed by transform, and returns a document-term matrix.\n",
    "\n",
    "# A mapping of terms to feature indices.\n",
    "result = vectorizer.vocabulary_\n",
    "\n",
    "print(\"We have \", len(result), \" words in our Bag of Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# transforming into feature vectors for the learning model\n",
    "\n",
    "vectorizer.fit_transform(tweet_list).toarray()\n",
    "# `fit_transform` learns a list of feature name -> indices mappings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data into a Training Set and a Testing Set to grade the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length of y_train: 38019\nlength of tweet_text_train: 38019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into testing and training sets\n",
    "\n",
    "tweet_text = tweets.values\n",
    "\n",
    "y = dt_transformed['Toxicity'].values\n",
    "\n",
    "tweet_text_train, tweet_text_test, y_train, y_test = train_test_split(tweet_text, y, test_size=0.33, random_state=0, stratify=y)\n",
    "# random_state shuffles the data so that we don't accidently end up with biased data\n",
    "# stratify to help keep the proportion of y values through the training and test sets\n",
    "\n",
    "# checking the length to ensure that my samples sizes are the same\n",
    "print(\"length of y_train:\", len(y_train))\n",
    "print(\"length of tweet_text_train:\", len(tweet_text_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<38019x39771 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# creating  the feature vectors in the training set and testing set.\n",
    "\n",
    "tweet_text_train = [inner[0] for inner in tweet_text_train]\n",
    "tweet_text_test = [inner[0] for inner in tweet_text_test]\n",
    "\n",
    "X_train = vectorizer.transform(tweet_text_train)\n",
    "X_test  = vectorizer.transform(tweet_text_test)\n",
    "X_train\n",
    "\n",
    "# we have compressed the vectorized data of 6631 elements into a format that takes up less space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score :  61%\n",
      "/Users/hexking/Desktop/my_projects/Threat_Detectives/ThreatDetectives/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LogisticRegression gives our training model a grade based off it's performance on the testing set\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Score : \", str(round(score * 100)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file = 'finalized_model.pkl'\n",
    "# saving the model to a pickled file to be copied into the back-end repo\n",
    "s = pickle.dumps(classifier)\n",
    "with open(pickle_file, \"wb\") as file:\n",
    "    file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_file = 'vectorizer_pickle.pkl'\n",
    "s = pickle.dumps(vectorizer)\n",
    "with open(vectorizer_file, \"wb\") as file:\n",
    "    file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# testing the pickled file \n",
    "with open(pickle_file, \"rb\") as file:\n",
    "    Pickled_Classifier = pickle.load(file)\n",
    "\n",
    "Pickled_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "with open(vectorizer_file, \"rb\") as file:\n",
    "    Pickled_vectorizer = pickle.load(file)\n",
    "\n",
    "Pickled_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3c8ebdc084ded4a9f5aa05e2a58f9e0f38e67a67191d5b8c86cebd9925662ff5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}